import os

from tokenizers import Tokenizer
from tokenizers.models import BPE, Unigram, WordLevel
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer
from transformers import PreTrainedTokenizerFast

# TODO: Replace example, generated by AI token txt files with actual ones
# ! Current Data for token is AI


def train_tokenizer(
    type: str = "bpe", training_directory: str = "data/training/", output_file_directory: str = "data/tokenizer.json"
):
    if not (training_directory.endswith("/")):
        training_directory += "/"

    if type == "bpe":
        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

        trainer = BpeTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
        tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens
    elif type == "wordpiece":
        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

        trainer = BpeTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    elif type == "unigram":
        tokenizer = Tokenizer(Unigram())

        trainer = UnigramTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens

    elif type == "wordlevel":
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))

        trainer = WordLevelTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    else:
        raise ValueError("Invalid tokenizer type")

    tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)  # Saves to token.json

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer
